import requests
import re
import html
import json
import xml.etree.ElementTree as ET
from ftfy import fix_text
import io
import csv
import datetime
import os
import argparse
import pandas as pd
from flask import Flask, request, render_template, jsonify, send_from_directory
import tempfile
import threading
import uuid
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Table, Paragraph, Spacer, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
import logging
from queue import Queue
import zipfile
import shutil # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏•‡∏ö directory
import time # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö threading.Timer ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ cleanup

app = Flask(__name__)

# --- ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞ Lock ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Thread-safe ---
processing_status = {}
status_lock = threading.Lock()

# --- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Logger ‡πÅ‡∏•‡∏∞ Log Queue ---
log_queue = Queue() # ‡∏™‡∏£‡πâ‡∏≤‡∏á Queue ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö log

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO) # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏£‡∏∞‡∏î‡∏±‡∏ö log ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å

# ‡∏•‡∏ö handler ‡πÄ‡∏Å‡πà‡∏≤‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ã‡πâ‡∏≥‡πÄ‡∏°‡∏∑‡πà‡∏≠ reload (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Flask dev server)
if logger.handlers:
    for handler in list(logger.handlers):
        logger.removeHandler(handler)

class QueueHandler(logging.Handler):
    """
    Handler ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡πà‡∏á log record ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Queue
    """
    def __init__(self, queue):
        super().__init__()
        self.queue = queue

    def emit(self, record):
        try:
            # ONLY send the formatted message to the queue, without full log details
            # This is where we control what goes to the frontend
            msg = self.format(record)
            
            # --- IMPORTANT: Custom formatting for frontend logs ---
            # Remove timestamp, level, and job ID from logs sent to frontend for clarity
            # Example: "2025-07-17 08:54:55,123 - INFO - Job f17846f3-4d62-44d9-a0a1-0176c18acd5c: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• NodeID: 185271..."
            # We want just: "‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• NodeID: 185271..."
            log_pattern = re.compile(r"^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3} - (INFO|WARNING|ERROR|CRITICAL) - (Job [0-9a-f-]+: )?(.*)")
            match = log_pattern.match(msg)
            if match:
                clean_msg = match.group(3) # Get the message part
                self.queue.put(clean_msg)
            else:
                self.queue.put(msg) # Fallback if pattern doesn't match
            # --- End Custom formatting ---

        except Exception:
            self.handleError(record)

# Console Handler (‡πÅ‡∏™‡∏î‡∏á log ‡πÉ‡∏ô Terminal) - ‡πÄ‡∏Å‡πá‡∏ö format ‡πÄ‡∏ï‡πá‡∏°‡πÑ‡∏ß‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö debugging
console_handler = logging.StreamHandler()
console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger.addHandler(console_handler)

# Queue Handler (‡∏™‡πà‡∏á log ‡πÑ‡∏õ‡∏¢‡∏±‡∏á Queue ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Frontend)
queue_handler = QueueHandler(log_queue)
# ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á formatter ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡∏à‡∏∞ format ‡πÄ‡∏≠‡∏á‡πÉ‡∏ô emit()
logger.addHandler(queue_handler)

# --- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ü‡∏≠‡∏ô‡∏ï‡πå‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PDF ---
THAI_FONT_NAME = 'THSarabunNew'
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ 'THSarabunNew.ttf' ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô directory ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏Å‡∏±‡∏ö app.py
THAI_FONT_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'THSarabunNew.ttf')

THAI_FONT_REGISTERED = False
if os.path.exists(THAI_FONT_PATH):
    try:
        pdfmetrics.registerFont(TTFont(THAI_FONT_NAME, THAI_FONT_PATH))
        THAI_FONT_REGISTERED = True
        logger.info(f"Thai font '{THAI_FONT_NAME}' registered successfully from '{THAI_FONT_PATH}'.")
    except Exception as e:
        logger.error(f"ERROR: Could not register Thai font '{THAI_FONT_NAME}'. Error: {e}")
else:
    logger.warning(f"WARNING: Thai font file '{THAI_FONT_PATH}' not found. Please ensure the font file is in the same directory as the script.")

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
def get_data_from_api(nod_id, itf_id, job_id):
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô JSON"""
    url = "http://1.179.233.116:8082/api_csoc_02/server_solarwinds_gin.php"
    headers = {
        "Content-Type": "text/xml; charset=utf-8",
        "SOAPAction": "http://1.179.233.116/api_csoc_02/server_solarwinds_gin.php/circuitStatus"
    }
    body = f"""<?xml version="1.1" encoding="utf-8"?>
<soap:Envelope xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/"
               xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
               xmlns:xsd="http://www.w3.org/2001/XMLSchema">
  <soap:Body>
    <circuitStatus xmlns="http://1.179.233.116/soap/#Service_Solarwinds_gin">
      <nodID>{nod_id}</nodID>
      <itfID>{itf_id}</itfID>
    </circuitStatus>
  </soap:Body>
</soap:Envelope>"""

    try:
        resp = requests.post(url, data=body, headers=headers, timeout=10)
        resp.raise_for_status()
        match = re.search(r"(<\?xml.*?</SOAP-ENV:Envelope>)", resp.text, re.DOTALL)
        if not match:
            logger.warning(f"‡πÑ‡∏°‡πà‡∏û‡∏ö XML Response ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}")
            return None
        
        root = ET.fromstring(match.group(1))
        return_tag = root.find(".//{*}return")
        if return_tag is None or not return_tag.text:
            logger.warning(f"API ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}")
            return None

        raw_text = return_tag.text
        html_unescaped = html.unescape(raw_text)
        fixed_text = fix_text(bytes(html_unescaped, "utf-8").decode("unicode_escape"))
        parsed_json = json.loads(fixed_text)
        return parsed_json
    except requests.exceptions.RequestException as req_e:
        logger.error(f"‚ùå ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NodeID: {nod_id}, Interface ID: {itf_id} ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {req_e}")
        return None
    except ET.ParseError as parse_e:
        logger.error(f"‚ùå XML Parsing ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}: {parse_e}")
        return None
    except json.JSONDecodeError as json_e:
        logger.error(f"‚ùå JSON Decoding ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}: {json_e}")
        return None
    except Exception as e:
        logger.error(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}: {e}")
        return None

def process_json_data(raw_json_data, job_id):
    """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• JSON ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå"""
    column_mapping = {
        "‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô": "Customer_Curcuit_ID",
        "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô": "Address",
        "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤": "Timestamp",
        "‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)": "Bandwidth",
        "‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô incoming (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)": "In_Averagebps",
        "‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô outcoming (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)": "Out_Averagebps"
    }
    desired_headers_th = list(column_mapping.keys())
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ raw_json_data ‡πÄ‡∏õ‡πá‡∏ô list ‡∏´‡∏£‡∏∑‡∏≠ dict
    data_to_process = raw_json_data if isinstance(raw_json_data, list) else [raw_json_data]

    # --- Step 1: Parse all existing timestamps and find earliest/latest dates ---
    formatted_data = []
    earliest_json_date = None
    latest_json_date = None

    for item in data_to_process:
        date_time_value = item.get("Timestamp")
        if isinstance(date_time_value, dict) and 'date' in date_time_value:
            try:
                # Parse timestamp from JSON string. Keep original format to reconstruct for output
                dt_obj = datetime.datetime.strptime(date_time_value['date'], '%Y-%m-%d %H:%M:%S.%f')
                formatted_item = item.copy()
                formatted_item['Parsed_Timestamp'] = dt_obj
                formatted_data.append(formatted_item)

                if earliest_json_date is None or dt_obj < earliest_json_date:
                    earliest_json_date = dt_obj
                if latest_json_date is None or dt_obj > latest_json_date:
                    latest_json_date = dt_obj
            except ValueError:
                # If date parsing fails, just add the item without a parsed timestamp
                # These items won't be considered for filling gaps
                formatted_data.append(item.copy()) 
                logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ parse ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ: {date_time_value.get('date')}. ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•.")

        else:
            # If 'Timestamp' is not a dict or 'date' is missing, add as is
            formatted_data.append(item.copy())

    # If no valid dates were found, just process the raw data as is
    if earliest_json_date is None or latest_json_date is None:
        logger.warning(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ô JSON ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏°‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà/‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡πÑ‡∏õ")
        processed_data = []
        for item in data_to_process: # Use original data_to_process to ensure all items are included
            row_data = {}
            for th_header, json_key in column_mapping.items():
                value = item.get(json_key, '')
                if th_header in ["‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô incoming (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)", "‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô outcoming (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)"]:
                    try:
                        value_float = float(value)
                        row_data[th_header] = f"{int(value_float):,}" # Format with comma
                    except (ValueError, TypeError):
                        row_data[th_header] = str(value)
                elif th_header == "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤" and isinstance(value, dict) and 'date' in value:
                    try:
                        dt_obj = datetime.datetime.strptime(value['date'], '%Y-%m-%d %H:%M:%S.%f')
                        row_data[th_header] = dt_obj.strftime('%Y-%m-%d %H.%M.%S')
                    except ValueError:
                        row_data[th_header] = str(value)
                elif th_header == "‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)":
                    if "FTTx" in str(value):
                        row_data[th_header] = "20 Mbps." # Changed from 0 Mbps.
                    else:
                        try:
                            numeric_value = float(re.search(r'[\d.]+', str(value)).group())
                            row_data[th_header] = f"{int(numeric_value):,} Mbps." # Format with comma
                        except (ValueError, TypeError, AttributeError):
                            row_data[th_header] = str(value)
                else:
                    row_data[th_header] = str(value)
            processed_data.append(row_data)
        return desired_headers_th, processed_data, {} # Return empty averages

    # --- Step 2: Determine the full time range for filling ---
    first_day_of_month_start_hour = earliest_json_date.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
    # The end point for filling is the last hour of the latest_json_date
    # We want to fill up to and including the last full hour of the latest data.
    # If latest_json_date is 2025-07-13 10:30:00, we want to fill up to 2025-07-13 23:00:00
    last_day_of_data_end_hour = latest_json_date.replace(hour=23, minute=0, second=0, microsecond=0)


    # --- Step 3: Get common data for new entries (customer ID, name, bandwidth) ---
    first_actual_entry = None
    for item in formatted_data:
        if 'Parsed_Timestamp' in item: # Find the first entry that actually has a parsed timestamp
            first_actual_entry = item
            break
    
    # Use default empty strings if no valid first entry is found
    customer_id = first_actual_entry.get("Customer_Curcuit_ID", "") if first_actual_entry else ""
    customer_name = first_actual_entry.get("Address", "") if first_actual_entry else ""
    bandwidth = first_actual_entry.get("Bandwidth", "") if first_actual_entry else ""

    # --- Step 4: Create a set of existing date-hour combinations for quick lookup ---
    existing_date_hours = set()
    for item in formatted_data:
        if 'Parsed_Timestamp' in item:
            existing_date_hours.add(item['Parsed_Timestamp'].replace(minute=0, second=0, microsecond=0)) # Store only year, month, day, hour

    # --- Step 5: Generate and fill in missing entries ---
    dates_to_add_data = []
    current_hour_dt = first_day_of_month_start_hour

    # Loop from the first hour of the month to the last hour of the latest data day
    while current_hour_dt <= last_day_of_data_end_hour:
        if current_hour_dt not in existing_date_hours:
            # Create a new entry for the missing hour
            missing_entry = {
                "Customer_Curcuit_ID": customer_id,
                "Address": customer_name,
                "Timestamp": {"date": current_hour_dt.strftime('%Y-%m-%d %H:%M:%S.%f')},
                "Bandwidth": bandwidth,
                "In_Averagebps": "0",
                "Out_Averagebps": "0",
                "Parsed_Timestamp": current_hour_dt
            }
            dates_to_add_data.append(missing_entry)
            logger.info(f"‚ú® ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡πÑ‡∏õ: {current_hour_dt.strftime('%Y-%m-%d %H:%M')}")
        current_hour_dt += datetime.timedelta(hours=1) # Move to the next hour
    
    # --- Step 6: Combine all data and sort by timestamp ---
    combined_data = dates_to_add_data + formatted_data
    
    # Ensure all data is sorted by the parsed timestamp
    # Filter out items that might not have a Parsed_Timestamp due to parsing errors
    sorted_combined_data = sorted(
        [item for item in combined_data if 'Parsed_Timestamp' in item],
        key=lambda x: x['Parsed_Timestamp']
    )

    # --- Step 7: Calculate monthly averages for In_Averagebps and Out_Averagebps ---
    total_sum_in = 0
    total_count_in = 0
    total_sum_out = 0
    total_count_out = 0

    for item in sorted_combined_data:
        try:
            in_bps = float(item.get("In_Averagebps", 0))
            total_sum_in += in_bps
            total_count_in += 1
        except (ValueError, TypeError):
            pass # Ignore non-numeric values

        try:
            out_bps = float(item.get("Out_Averagebps", 0))
            total_sum_out += out_bps
            total_count_out += 1
        except (ValueError, TypeError):
            pass # Ignore non-numeric values
    
    monthly_averages = {}
    if total_count_in > 0 and total_count_out > 0:
        avg_in_month = (total_sum_in / total_count_in)
        avg_out_month = (total_sum_out / total_count_out)
        monthly_averages = {
            "avg_in_month": int(avg_in_month),
            "avg_out_month": int(avg_out_month)
        }

    # --- Step 8: Final formatting for output ---
    processed_data = []
    for item in sorted_combined_data:
        row_data = {}
        for th_header, json_key in column_mapping.items():
            # For '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤', use the 'Parsed_Timestamp' for consistent formatting
            if th_header == "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤":
                row_data[th_header] = item['Parsed_Timestamp'].strftime('%Y-%m-%d %H.%M.%S')
            else:
                value = item.get(json_key, '')
                if th_header in ["‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô incoming (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)", "‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô outcoming (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)"]:
                    try:
                        value_float = float(value)
                        row_data[th_header] = f"{int(value_float):,}" # Format with comma
                    except (ValueError, TypeError):
                        row_data[th_header] = str(value)
                elif th_header == "‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)":
                    if "FTTx" in str(value):
                        row_data[th_header] = "20 Mbps." # Changed from 0 Mbps.
                    else:
                        try:
                            numeric_value = float(re.search(r'[\d.]+', str(value)).group())
                            row_data[th_header] = f"{int(numeric_value):,} Mbps." # Format with comma
                        except (ValueError, TypeError, AttributeError):
                            row_data[th_header] = str(value)
                else:
                    row_data[th_header] = str(value)
        processed_data.append(row_data)

    return desired_headers_th, processed_data, monthly_averages

def export_to_csv(headers, data, monthly_averages, filename, job_id, node_name):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ '‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô' ‡πÅ‡∏•‡∏∞ '‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô' ‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡πÅ‡∏ñ‡∏ß
       ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
       
       ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:
       - '‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô incoming (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)' ‡πÄ‡∏õ‡πá‡∏ô 'In_Averagebps'
       - '‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô outcoming (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)' ‡πÄ‡∏õ‡πá‡∏ô 'Out_Averagebps'
    """
    try:
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            cw = csv.writer(f)
            if headers and data:
                # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô CSV
                csv_display_headers = [
                    "‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô",
                    "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô",
                    "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤",
                    "‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)",
                    "In_Averagebps",  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                    "Out_Averagebps"  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á
                ]
                cw.writerow(csv_display_headers) # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà‡∏•‡∏á‡πÑ‡∏õ
                
                for row in data:
                    new_row = [
                        row.get('‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', ''),
                        row.get('‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', ''),
                        row.get('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤', ''),
                        row.get('‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)', ''),
                        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏Ñ‡∏µ‡∏¢‡πå‡πÄ‡∏î‡∏¥‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô 'data' ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
                        row.get('‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô incoming (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)', ''),
                        row.get('‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô outcoming (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)', '')
                    ]
                    cw.writerow(new_row)
                
                # Insert monthly average row at the very end
                if monthly_averages:
                    avg_in = monthly_averages['avg_in_month']
                    avg_out = monthly_averages['avg_out_month']
                    cw.writerow([
                        '', '', # Empty for customer ID/name
                        'Total', 
                        '', # Bandwidth
                        f'{avg_in:,}', 
                        f'{avg_out:,}'
                    ])
            else:
                cw.writerow(["No Data"])
        logger.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á CSV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{node_name}' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß")
        return True, "Success"
    except Exception as e:
        logger.error(f"‚ùå ‡∏™‡∏£‡πâ‡∏≤‡∏á CSV ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{node_name}' ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
        return False, str(e)


def export_to_pdf(headers, data, monthly_averages, filename, job_id, node_name):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå PDF ‡πÇ‡∏î‡∏¢‡πÉ‡∏´‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡∏°‡πà ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢"""
    try:
        doc = SimpleDocTemplate(filename, pagesize=letter)
        styles = getSampleStyleSheet()
        elements = []

        # Constants for page dimensions
        # letter size is 8.5 x 11 inches. 1 inch = 72 points
        page_width, page_height = letter
        left_margin = right_margin = 0.5 * inch # Set a consistent margin

        # Calculate available width for the table
        available_width = page_width - (left_margin + right_margin)

        if headers and data:
            data_by_date = {}
            for row in data:
                date_time_str = row.get('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤', '')
                try:
                    date_key = datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H.%M.%S').strftime('%Y-%m-%d')
                except ValueError:
                    date_key = 'Uncategorized' # Fallback for malformed date
                if date_key not in data_by_date:
                    data_by_date[date_key] = []
                data_by_date[date_key].append(row)
            
            sorted_date_keys = sorted(data_by_date.keys())
            last_date_key = sorted_date_keys[-1] if sorted_date_keys else None

            first_page = True
            for i, date_key in enumerate(sorted_date_keys):
                group_data = data_by_date[date_key]

                if not first_page:
                    elements.append(PageBreak())
                
                # Title
                title_style = styles['Title']
                if THAI_FONT_REGISTERED:
                    title_style.fontName = THAI_FONT_NAME
                title_style.fontSize = 18
                title_style.alignment = 1
                elements.append(Paragraph("Custumer Interface Summary Report by Hour", title_style))
                elements.append(Spacer(1, 0.2 * inch))

                # Subtitle (Date)
                sub_title_style = ParagraphStyle('SubTitle', parent=styles['Normal'])
                if THAI_FONT_REGISTERED:
                    sub_title_style.fontName = THAI_FONT_NAME
                sub_title_style.fontSize = 12
                sub_title_style.alignment = 1 
                elements.append(Paragraph(f"<b>‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà:</b> {date_key}", sub_title_style))
                elements.append(Spacer(1, 0.2 * inch))

                table_headers = [
                    "‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô",
                    "‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô",
                    "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤",
                    "‡∏Ç‡∏ô‡∏≤‡∏îBandwidth \n(‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)",
                    "‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô incoming\n (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)",
                    "‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô outcoming\n (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)"
                ]
                
                table_data = [table_headers]
                last_customer_id = None
                last_customer_name = None
                
                for row in group_data:
                    current_customer_id = row.get('‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', '')
                    current_customer_name = row.get('‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', '')
                    display_customer_id = current_customer_id if current_customer_id != last_customer_id else ''
                    display_customer_name = current_customer_name if current_customer_name != last_customer_name else ''
                    table_data.append([
                        display_customer_id,
                        display_customer_name,
                        row.get('‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤', ''),
                        row.get('‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (‡∏´‡∏ô‡πà‡∏ß‡∏¢ Mbps)', ''),
                        row.get('‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô incoming (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)', ''),
                        row.get('‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô outcoming (‡∏´‡∏ô‡πà‡∏ß‡∏¢ bps)', '')
                    ])
                    last_customer_id = current_customer_id
                    last_customer_name = current_customer_name
                
                # Check if this is the last day's data and monthly averages exist
                is_last_day_group = (date_key == last_date_key)
                if is_last_day_group and monthly_averages:
                    avg_in = monthly_averages['avg_in_month']
                    avg_out = monthly_averages['avg_out_month']
                    
                    # Create a Paragraph for the average text to allow styling
                    average_text_style = ParagraphStyle('AverageText', parent=styles['Normal'])
                    if THAI_FONT_REGISTERED:
                        average_text_style.fontName = THAI_FONT_NAME
                    average_text_style.alignment = 0 # Left align in the cell
                    average_text_style.fontSize = 10 # Match table body font size

                    table_data.append([
                        '', # Empty cell for '‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô'
                        '', # Empty cell for '‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô'
                        Paragraph("<b>Total</b>", average_text_style), # Bold average text
                        '', # Empty cell for '‡∏Ç‡∏ô‡∏≤‡∏îBandwidth'
                        Paragraph(f"<b>{avg_in:,}</b>", average_text_style), # Bold and comma-formatted In_Averagebps
                        Paragraph(f"<b>{avg_out:,}</b>", average_text_style)  # Bold and comma-formatted Out_Averagebps
                    ])
                
                # Define column widths as percentages of available_width
                # This ensures the table expands to fill the page width
                # Total percentages should add up to 1.0 or 100%
                col_widths = [
                    0.10 * available_width,  # ‡∏£‡∏´‡∏±‡∏™‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô (Customer ID)
                    0.29 * available_width,  # ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô (Customer Name)
                    0.13 * available_width,  # ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤ (Date and Time)
                    0.13 * available_width,  # ‡∏Ç‡∏ô‡∏≤‡∏îBandwidth (Bandwidth)
                    0.18 * available_width,  # ‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô incoming (In usage)
                    0.18 * available_width   # ‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô outcoming (Out usage)
                ]
                
                table = Table(table_data, colWidths=col_widths)
                
                table_style = [
                    ('BACKGROUND', (0, 0), (-1, 0), '#cccccc'),
                    ('TEXTCOLOR', (0, 0), (-1, 0), '#000000'),
                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                    ('BACKGROUND', (0, 1), (-1, -1), '#f0f0f0'), 
                    ('GRID', (0, 0), (-1, -1), 1, '#999999'),
                    ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'), # Vertically align text in cells
                ]
                if THAI_FONT_REGISTERED:
                    table_style.append(('FONTNAME', (0, 0), (-1, 0), THAI_FONT_NAME)) # Headers in Thai font
                    table_style.append(('FONTNAME', (0, 1), (-1, -1), THAI_FONT_NAME)) # Body in Thai font
                else:
                    table_style.append(('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold')) # Headers in bold
                    table_style.append(('FONTNAME', (0, 1), (-1, -1), 'Helvetica')) # Body in regular

                # Apply specific style for the last row if it's the monthly average
                if is_last_day_group and monthly_averages:
                    last_row_index = len(table_data) - 1
                    table_style.extend([
                        ('BACKGROUND', (0, last_row_index), (-1, last_row_index), '#d3d3d3'), # Light grey background for average row
                        ('SPAN', (0, last_row_index), (1, last_row_index)), # Span first two columns
                        ('ALIGN', (2, last_row_index), (2, last_row_index), 'LEFT'), # Align 'Total' text left
                        ('FONTNAME', (2, last_row_index), (2, last_row_index), THAI_FONT_NAME if THAI_FONT_REGISTERED else 'Helvetica-Bold'),
                        ('FONTNAME', (4, last_row_index), (4, last_row_index), THAI_FONT_NAME if THAI_FONT_REGISTERED else 'Helvetica-Bold'),
                        ('FONTNAME', (5, last_row_index), (5, last_row_index), THAI_FONT_NAME if THAI_FONT_REGISTERED else 'Helvetica-Bold'),
                        ('ALIGN', (4, last_row_index), (4, last_row_index), 'CENTER'), # Center average values
                        ('ALIGN', (5, last_row_index), (5, last_row_index), 'CENTER'), # Center average values
                        ('BOTTOMPADDING', (0, last_row_index), (-1, last_row_index), 12),
                        ('TOPPADDING', (0, last_row_index), (-1, last_row_index), 12),
                    ])

                table.setStyle(table_style)
                elements.append(table)
                elements.append(Spacer(1, 0.5 * inch))

                first_page = False
        else:
            no_data_style = styles['Normal']
            if THAI_FONT_REGISTERED:
                no_data_style.fontName = THAI_FONT_NAME
            elements.append(Paragraph("No circuit status data available.", no_data_style))
        
        # Build the document with the defined margins
        doc.leftMargin = left_margin
        doc.rightMargin = right_margin
        doc.topMargin = 0.5 * inch # Set top margin
        doc.bottomMargin = 0.5 * inch # Set bottom margin

        doc.build(elements)
        logger.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á PDF ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{node_name}' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß")
        return True, "PDF generated successfully."
    except Exception as e:
        logger.error(f"‚ùå ‡∏™‡∏£‡πâ‡∏≤‡∏á PDF ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö '{node_name}' ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
        return False, f"Error generating PDF: {e}"

def process_file_in_background(file_stream, job_id):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô‡∏≠‡∏µ‡∏Å Thread ‡∏´‡∏ô‡∏∂‡πà‡∏á
    ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡∏£‡∏±‡∏ö file_stream (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ü‡∏•‡πå) ‡πÅ‡∏•‡∏∞ job_id ‡∏°‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
    """
    temp_dir = None # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CSV/PDF ‡∏¢‡πà‡∏≠‡∏¢
    try:
        df = pd.read_excel(file_stream)
        total_rows = len(df)
        with status_lock:
            processing_status[job_id]['total'] = total_rows
            processing_status[job_id]['results'] = []
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á directory ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå CSV/PDF ‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô‡∏ô‡∏µ‡πâ
            temp_dir = tempfile.mkdtemp(prefix=f"report_job_{job_id}_")
            processing_status[job_id]['temp_dir'] = temp_dir # ‡πÄ‡∏Å‡πá‡∏ö temp_dir ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
        
        logger.info(f"üìä ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå Excel ‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {total_rows} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

        required_columns = ['NodeID', 'Interface ID', '‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á / ‡∏™‡∏±‡∏á‡∏Å‡∏±‡∏î', '‡∏Å‡∏£‡∏° / ‡∏™‡∏±‡∏á‡∏Å‡∏±‡∏î', '‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î', '‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', 'Node Name']
        if not all(col in df.columns for col in required_columns):
            missing_cols = [c for c in required_columns if c not in df.columns]
            with status_lock:
                processing_status[job_id]['error'] = f"‡πÑ‡∏ü‡∏•‡πå Excel ‡∏Ç‡∏≤‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô: {', '.join(missing_cols)}"
                processing_status[job_id]['completed'] = True
            logger.error(f"‚ùå {processing_status[job_id]['error']}")
            return
        
        # ‡πÉ‡∏ä‡πâ temp_dir ‡πÄ‡∏õ‡πá‡∏ô root ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
        csv_root_dir = os.path.join(temp_dir, 'csv')
        pdf_root_dir = os.path.join(temp_dir, 'pdf')
        os.makedirs(csv_root_dir, exist_ok=True)
        os.makedirs(pdf_root_dir, exist_ok=True)
        
        for index, row in df.iterrows():
            with status_lock:
                if processing_status[job_id].get('canceled'):
                    logger.info(f"‚õî ‡∏á‡∏≤‡∏ô‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡πÇ‡∏î‡∏¢‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ")
                    break
            
            node_name = ''
            csv_success = False
            pdf_success = False
            error_message = None

            try:
                nod_id = str(row['NodeID']).strip()
                itf_id = str(row['Interface ID']).strip()
                
                folder1 = str(row['‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á / ‡∏™‡∏±‡∏á‡∏Å‡∏±‡∏î']).strip()
                folder2 = str(row['‡∏Å‡∏£‡∏° / ‡∏™‡∏±‡∏á‡∏Å‡∏±‡∏î']).strip()
                folder3 = str(row['‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î']).strip()
                folder4 = str(row['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô']).strip()
                node_name = str(row['Node Name']).strip()

                if not nod_id or not itf_id:
                    error_message = "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• NodeID ‡∏´‡∏£‡∏∑‡∏≠ Interface ID ‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå"
                    logger.warning(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà {index + 1} ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å {error_message} (NodeID: '{nod_id}', ITF ID: '{itf_id}')")
                    with status_lock:
                        processing_status[job_id]['processed'] += 1
                        processing_status[job_id]['results'].append({
                            'node_name': node_name,
                            'csv_success': False,
                            'pdf_success': False,
                            'error_message': error_message
                        })
                    continue
                
                logger.info(f"‚ñ∂ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• NodeID: {nod_id}, Interface ID: {itf_id} (‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà {index + 1})")

                # ‡∏™‡∏£‡πâ‡∏≤‡∏á sub-directory ‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ temp_dir
                current_csv_dir = os.path.join(csv_root_dir, folder1, folder2, folder3, folder4)
                current_pdf_dir = os.path.join(pdf_root_dir, folder1, folder2, folder3, folder4)
                
                os.makedirs(current_csv_dir, exist_ok=True)
                os.makedirs(current_pdf_dir, exist_ok=True)
                
                raw_json_data = get_data_from_api(nod_id, itf_id, job_id)

                if raw_json_data:
                    headers, processed_data, monthly_averages = process_json_data(raw_json_data, job_id)
                    
                    # Clean node_name for filenames
                    sanitized_node_name = re.sub(r'[\\/:*?"<>|]', '_', node_name)
                    # MODIFIED: ‡πÄ‡∏û‡∏¥‡πà‡∏° ID ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà Node Name ‡∏ã‡πâ‡∏≥
                    # OLD: filename_base = f"{sanitized_node_name}_{nod_id}_{itf_id}" 
                    # NEW: ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏Ñ‡πà Node Name
                    filename_base = f"{sanitized_node_name}" 

                    csv_filename = os.path.join(current_csv_dir, f"{filename_base}.csv")
                    pdf_filename = os.path.join(current_pdf_dir, f"{filename_base}.pdf")

                    csv_success, csv_msg = export_to_csv(headers, processed_data, monthly_averages, csv_filename, job_id, node_name)
                    pdf_success, pdf_msg = export_to_pdf(headers, processed_data, monthly_averages, pdf_filename, job_id, node_name)
                else:
                    error_message = f"‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å API ‡πÑ‡∏î‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NodeID: {nod_id}, Interface ID: {itf_id}"
                    logger.error(f"‚ùå {error_message}")
            
            except Exception as e:
                error_message = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏≤‡∏î‡∏Ñ‡∏¥‡∏î‡πÉ‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà {index + 1}: {e}"
                logger.error(f"‚ùå {error_message}")
                
            finally:
                with status_lock:
                    processing_status[job_id]['processed'] += 1
                    processing_status[job_id]['results'].append({
                        'node_name': node_name,
                        'csv_success': csv_success,
                        'pdf_success': pdf_success,
                        'error_message': error_message
                    })
        
        # ‡∏´‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå ZIP
        if not processing_status[job_id].get('canceled'):
            zip_filename = f"customer_reports_{job_id}_{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}.zip"
            # ‡πÉ‡∏´‡πâ zip_file_path ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà tempfile.gettempdir() ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            zip_file_path = os.path.join(tempfile.gettempdir(), zip_filename)
            
            if temp_dir and os.path.exists(temp_dir):
                with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for root, _, files in os.walk(temp_dir):
                        for file in files:
                            file_path = os.path.join(root, file)
                            # ‡∏™‡∏£‡πâ‡∏≤‡∏á relative path ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô zip (‡∏ï‡∏±‡∏î temp_dir ‡∏≠‡∏≠‡∏Å)
                            arcname = os.path.relpath(file_path, temp_dir)
                            zipf.write(file_path, arcname)
                
                with status_lock:
                    processing_status[job_id]['zip_file_path'] = zip_file_path
                    processing_status[job_id]['completed'] = True
                logger.info(f"‚úÖ ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå! ‡πÑ‡∏ü‡∏•‡πå ZIP: {zip_file_path.split(os.sep)[-1]}")
            else:
                with status_lock:
                    processing_status[job_id]['error'] = "‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á ZIP"
                    processing_status[job_id]['completed'] = True
                logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß '{temp_dir}' ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á ZIP ‡πÑ‡∏î‡πâ")
        else:
            with status_lock:
                 processing_status[job_id]['completed'] = True
                 processing_status[job_id]['error'] = "‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å"

    except Exception as e:
        with status_lock:
            processing_status[job_id]['error'] = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á: {e}"
            processing_status[job_id]['completed'] = True
        logger.critical(f"‚ùå {processing_status[job_id]['error']}")
    finally:
        # **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:** ‡∏•‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CSV/PDF (temp_dir)
        # ‡πÑ‡∏ü‡∏•‡πå ZIP ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô tempfile.gettempdir() ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏•‡∏ö‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
        if temp_dir and os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir, ignore_errors=True)
                logger.info(f"üìÅ ‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå CSV/PDF ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß: {temp_dir.split(os.sep)[-1]} ‡πÅ‡∏•‡πâ‡∏ß (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå ZIP)")
            except Exception as e:
                logger.error(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå CSV/PDF ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡πâ‡∏≤‡∏á: {e}")
        
        # *** ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏•‡∏ö job_id ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å processing_status ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡πÅ‡∏•‡πâ‡∏ß ***
        # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå ZIP ‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ
        # ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÅ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå ZIP ‡πÄ‡∏Å‡πà‡∏≤‡πÜ ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡πÉ‡∏ô‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á
        # ‡∏î‡∏π‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `cleanup_old_jobs` ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á


# --- Route ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Flask App ---
@app.route('/')
def upload_form():
    """‡πÅ‡∏™‡∏î‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå Excel"""
    return render_template('index.html')

@app.route('/generate_report', methods=['POST'])
def generate_report():
    """
    ‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î ‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÉ‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á
    """
    if 'excel_file' not in request.files:
        return jsonify({"error": "No file part"}), 400
    
    file = request.files['excel_file']
    if file.filename == '':
        return jsonify({"error": "No selected file"}), 400
    
    if file:
        job_id = str(uuid.uuid4())
        file_stream = io.BytesIO(file.read())
        
        with status_lock:
            processing_status[job_id] = {
                'total': -1, 
                'processed': 0, 
                'completed': False, 
                'error': None,
                'canceled': False,
                'results': [],
                'temp_dir': None,       # ‡πÄ‡∏Å‡πá‡∏ö directory ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CSV/PDF)
                'zip_file_path': None,  # ‡πÄ‡∏Å‡πá‡∏ö path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå zip
                'timestamp': datetime.datetime.now() # ‡πÄ‡∏û‡∏¥‡πà‡∏° timestamp ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï
            }
        logger.info(f"üìÇ ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå excel '{file.filename}' ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (Job ID: {job_id})")

        thread = threading.Thread(target=process_file_in_background, args=(file_stream, job_id))
        thread.daemon = True # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ thread ‡∏à‡∏ö‡πÄ‡∏°‡∏∑‡πà‡∏≠ process ‡∏´‡∏•‡∏±‡∏Å‡∏à‡∏ö
        thread.start()
        
        return jsonify({"message": "Processing started", "job_id": job_id})

@app.route('/status/<job_id>')
def get_status(job_id):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏≠‡∏¢‡∏π‡πà
    """
    with status_lock:
        status = processing_status.get(job_id, {})
    return jsonify(status)

@app.route('/logs/<job_id>')
def get_logs(job_id):
    """
    ‡∏î‡∏∂‡∏á log ‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏≠‡∏¢‡∏π‡πà
    """
    logs = []
    # ‡∏î‡∏∂‡∏á log ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å queue ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤
    while not log_queue.empty():
        try:
            logs.append(log_queue.get_nowait())
        except Exception:
            break
    return jsonify({"logs": logs})


@app.route('/cancel/<job_id>', methods=['POST'])
def cancel_job(job_id):
    """
    ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏≠‡∏¢‡∏π‡πà
    """
    with status_lock:
        if job_id in processing_status:
            processing_status[job_id]['canceled'] = True
            logger.info(f"‚õî ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏Ç‡∏≠‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏á‡∏≤‡∏ô (Job ID: {job_id})")
            return jsonify({"message": "Job cancellation requested"}), 200
        else:
            logger.warning(f"‚ö†Ô∏è ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏û‡∏ö (Job ID: {job_id})")
            return jsonify({"error": "Job not found"}), 404

@app.route('/download_report/<job_id>')
def download_report(job_id):
    """
    ‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå ZIP ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
    """
    with status_lock:
        job_info = processing_status.get(job_id)

    if not job_info:
        logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏á‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î (Job ID: {job_id})")
        return jsonify({"error": "Job not found or not ready for download. It might be too old or cancelled."}), 404

    zip_file_path = job_info.get('zip_file_path')

    if not zip_file_path or not os.path.exists(zip_file_path):
        logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå ZIP ‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à (Job ID: {job_id}). Path: {zip_file_path}")
        if job_info.get('completed') and not zip_file_path:
            return jsonify({"error": "Report completed with no ZIP file generated (internal error)"}), 500
        return jsonify({"error": "Report not yet generated or file not found"}), 404
    
    try:
        directory = tempfile.gettempdir()
        filename = os.path.basename(zip_file_path)
        logger.info(f"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå ZIP: {filename} ‡∏à‡∏≤‡∏Å {directory} (Job ID: {job_id})")
        
        # MODIFIED: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå ZIP ‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏à‡∏∞‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î
        current_date_str = datetime.datetime.now().strftime('%Y%m%d') # ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö ‡∏õ‡∏µ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ß‡∏±‡∏ô
        download_filename = f"Solarwind_{current_date_str}.zip"

        response = send_from_directory(
            directory=directory,
            path=filename,
            as_attachment=True,
            mimetype='application/zip',
            download_name=download_filename # ‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
        )
        
        return response

    except Exception as e:
        logger.critical(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå ZIP: {e} (Job ID: {job_id})")
        return jsonify({"error": f"Failed to serve file: {e}"}), 500

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤ (‡∏Ñ‡∏ß‡∏£‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Production) ---
def cleanup_old_jobs():
    """
    ‡∏•‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå ZIP ‡πÄ‡∏Å‡πà‡∏≤‡πÜ ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö
    ‡∏£‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô background process
    """
    logger.info("üßπ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏á‡∏≤‡∏ô‡πÄ‡∏Å‡πà‡∏≤...")
    current_time = datetime.datetime.now()
    jobs_to_remove = []

    retention_hours = 24  # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ü‡∏•‡πå (‡πÄ‡∏ä‡πà‡∏ô 24 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á) # ‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏¥‡∏™‡∏Å‡πå
    retention_seconds = retention_hours * 3600

    with status_lock:
        # ‡πÉ‡∏ä‡πâ list() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏≥‡πÄ‡∏ô‡∏≤‡∏Ç‡∏≠‡∏á keys/items ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô RuntimeError: dictionary changed size during iteration
        for job_id, job_info in list(processing_status.items()): 
            # ‡∏•‡∏ö‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
            # ‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏ö‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
            if job_info.get('completed') and job_info.get('timestamp'): 
                job_timestamp = job_info['timestamp']
                if (current_time - job_timestamp).total_seconds() > retention_seconds:
                    jobs_to_remove.append(job_id)
            elif (not job_info.get('completed')) and (current_time - job_info.get('timestamp', current_time)).total_seconds() > (retention_seconds / 4): # ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏•‡∏ö‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
                # ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° logic ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡πâ‡∏≤‡∏á‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡πÅ‡∏•‡∏∞‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
                logger.warning(f"‚ö†Ô∏è ‡∏û‡∏ö‡∏á‡∏≤‡∏ô‡∏Ñ‡πâ‡∏≤‡∏á‡πÄ‡∏Å‡πà‡∏≤ (‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå) ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ñ‡∏π‡∏Å‡∏•‡∏ö: {job_id}")
                jobs_to_remove.append(job_id)


    for job_id in jobs_to_remove:
        with status_lock:
            # ‡πÉ‡∏ä‡πâ .pop() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏ö key ‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å dictionary ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö‡πÑ‡∏î‡πâ value ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤
            job_info = processing_status.pop(job_id, None) 
        if job_info:
            zip_file_path = job_info.get('zip_file_path')
            if zip_file_path and os.path.exists(zip_file_path):
                try:
                    os.remove(zip_file_path)
                    logger.info(f"üóëÔ∏è ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå ZIP ‡πÄ‡∏Å‡πà‡∏≤: {os.path.basename(zip_file_path)} (Job ID: {job_id})")
                except Exception as e:
                    logger.error(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå ZIP ‡πÄ‡∏Å‡πà‡∏≤: {e} (Job ID: {job_id})")
            logger.info(f"‚ú® ‡∏•‡πâ‡∏≤‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏á‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Job ID: {job_id} ‡πÅ‡∏•‡πâ‡∏ß")
    logger.info("üßπ ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏á‡∏≤‡∏ô‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå")
    # ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà
    threading.Timer(retention_seconds / 2, cleanup_old_jobs).start() # ‡∏£‡∏±‡∏ô‡∏ö‡πà‡∏≠‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô ‡∏ó‡∏∏‡∏Å‡πÜ 12 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á)

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô Flask App ---
if __name__ == '__main__':
    cleanup_thread = threading.Thread(target=cleanup_old_jobs)
    cleanup_thread.daemon = True # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ thread ‡∏à‡∏ö‡πÄ‡∏°‡∏∑‡πà‡∏≠ process ‡∏´‡∏•‡∏±‡∏Å‡∏à‡∏ö
    cleanup_thread.start()

    app.run(debug=True) # debug=True ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤ ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Production